<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>青衫</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wallsbreaker.github.io/"/>
  <updated>2017-01-18T05:11:44.000Z</updated>
  <id>http://wallsbreaker.github.io/</id>
  
  <author>
    <name>青衫独醉</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SVM</title>
    <link href="http://wallsbreaker.github.io/2017/01/18/SVM/"/>
    <id>http://wallsbreaker.github.io/2017/01/18/SVM/</id>
    <published>2017-01-18T05:10:36.000Z</published>
    <updated>2017-01-18T05:11:44.000Z</updated>
    
    <content type="html">&lt;p&gt;SVM是一种无论从研究还是应用上都非常有价值的模型，它的效果非常好，尤其是 在解决小样本、非线性以及高纬数据中表现出了较之以前的方法不曾有的优势， 但是它的基本模型仍然是一个简单的线性分类模型，Max-margin的思想让它 区别于感知器，软间隔和核方法让SVM也同样适用于线性不可分的样本和非线性 模型。&lt;/p&gt;
&lt;h2 id=&quot;线性可分SVM&quot;&gt;&lt;a href=&quot;#线性可分SVM&quot; class=&quot;headerlink&quot; title=&quot;线性可分SVM&quot;&gt;&lt;/a&gt;线性可分SVM&lt;/h2&gt;&lt;p&gt;一般地，当样本线性可分的时候，存在无数个分离面使得正负样本可以正确分类， 比如感知器的目标是误分类的样本点最少，如下图示例，当许多分离面都可以 正确分类时，它便无所适从。抛开模型单凭人类直觉的话，其实我们都会 觉得红色的分类面（线，面，超平面在后续的描述中将不做特意区分）更好，但 具体好在什么地方，就需要静下心来慢慢谈了。SVM利用间隔最大化得思想就 使得它可以选择出这条红色的最好的分类面。这也从侧面告诉我们一个经验， 那就是能够更好解决问题的模型或结果不一定多么复杂，但往往都比较符合 人类的直觉。也是，机器学习不就是想去模拟学习人的思考过程吗。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/svm_1.png&quot; alt=&quot;二类分类问题图示&quot;&gt;&lt;/p&gt;
&lt;p&gt;首先我们来明确一下间隔的含义，在中学数学中我们经常说的距离：点到直线 的距离，点到面的距离，这里的距离称之为函数间隔。超平面$(w,b)$关于样本点 $(x_i, y_i)$的函数距离间隔为：&lt;br&gt;$$\hat{y_i}=|w \cdot x_i + b| $$&lt;br&gt;对于二类分类问题， 正例标签$y_i=1$负例标签$y_i=-1$，可以把上式写为:&lt;br&gt;$$\hat{y_i}=y_i(w \cdot x_i + b)$$&lt;br&gt;仅仅是函数间隔还是不够的，因为只要成比例的放大$w, b$，分类面不变，但函数 间隔却会放大，在间隔最大化的优化过程求解中，函数间隔的这个性质使得 它变得不适用。由此引出的几何间隔考虑了$w$的因素，则完全没有这个问题。 超平面$(w,b)$关于样本点 $(x_i, y_i)$的几何间隔：&lt;br&gt;$$y_i=\frac{1}{||w||}y_i(w \cdot x_i + b)$$&lt;br&gt;定义该超平面关于整个训练集$T$的几何间隔为所有样本点的最小值，即：&lt;br&gt;$$y = \min \quad y_i,i = 1,\cdot,N $$&lt;br&gt;对于二类的线性可分情况，SVM的基本想法就是求解能够正确划分训练数据集并且与支持向量几何间隔最大的 超平面，这个超平面是唯一的.几何间隔越大即意味着有更大的确信度来对训练数据 进行分类，这使得不仅能够正确分类，而且对于训练数据中最难分的实例也能以 尽量大的可信度来将他们分来，这不仅是对训练数据更好的解释，也提高了模型 对未来未知数据正确预测的能力。&lt;/p&gt;
&lt;h2 id=&quot;形式化描述&quot;&gt;&lt;a href=&quot;#形式化描述&quot; class=&quot;headerlink&quot; title=&quot;形式化描述&quot;&gt;&lt;/a&gt;形式化描述&lt;/h2&gt;&lt;p&gt;将最大化几何间隔表述为规划问题即：&lt;br&gt;$$\max \quad y\\&lt;br&gt;s.t. \quad \frac{1}{||w||} y_i (w \cdot x_i + b) \ge y, \quad i=1,\dots,N$$&lt;br&gt;我们知道几何间隔与函数间隔有个$||w||$倍的缩放，所以上式可以变形为:&lt;br&gt;$$\max \quad \frac{\hat{y}}{||w||}\\&lt;br&gt;s.t. \quad y_i (w \cdot x_i + b) \ge \hat{y}, \quad i=1,\dots,N$$&lt;br&gt;在约束条件中，我们要求所有样本点的函数距离都需要大于一个最小值，我们也知道 函数间隔的值是可以通过$w$来缩放调节的，所以这个最小值到底取值为多少对问题并没有影响，设$\hat{y}=1$，代入上式并对目标函数乘$\frac12$（为后续变形方便）：&lt;br&gt;$$\min \quad \frac12 ||w||^2\\&lt;br&gt;s.t. \quad y_i (w \cdot x_i + b) - 1 \ge 0, \quad i=1,\dots,N$$&lt;br&gt;到这里，我们就得到了线性二类分类SVM的形式化描述，需要注意的是，在约束条件中只有等号成立时的样本点才会起作用，这一点会在后续的对偶中详细解释，这些样本点即离分类超平面最近的一些点，称之为支持向量，SVM便是由这些“重要的”样本点确定的。&lt;br&gt;构造好了最优化问题，求得最优解$w^*,b^*$得到分离超平面：&lt;br&gt;$$w^* x + b^* = 0$$&lt;br&gt;分类决策函数：&lt;br&gt;$$f(x) = sign(w^* x + b^*)$$&lt;/p&gt;
&lt;h2 id=&quot;对偶学习算法&quot;&gt;&lt;a href=&quot;#对偶学习算法&quot; class=&quot;headerlink&quot; title=&quot;对偶学习算法&quot;&gt;&lt;/a&gt;对偶学习算法&lt;/h2&gt;&lt;p&gt;在上面我们得到了形式化的最优化问题表述，通过求解就能够得到分类决策函数，但是这样直接求解往往比较困难，于是通过拉格朗日对偶来求解对偶问题从而得到原始问题的最优解，因为对偶问题往往更容易求解，引入对偶的另外一个好处就是可能更方便的引入核函数。拉格朗日变换可得：&lt;br&gt;&lt;span&gt;$$\max_\alpha \min_{w,b} L(w,b,\alpha)\\
L(w,b,\alpha) = \frac12 ||w||^2 - \sum_{i=1|}^N \alpha_i y_i(wx_i+b)+\sum_{i=1}^N \alpha_i$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;首先求&lt;span&gt;$\min \limits_{w,b} L(w,b,\alpha)$&lt;/span&gt;&lt;!-- Has MathJax --&gt; 将拉格朗日函数分别对$w,b$求偏导并令其等于$0$得:&lt;br&gt;&lt;span&gt;$$w = \sum_{i=1}^N \alpha_i y_i x_i\\
\sum_{i=1}^N \alpha_i y_i = 0$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;代入得：&lt;br&gt;&lt;span&gt;$$\min \limits_{w,b} L(w,b,\alpha) = -\frac12 \sum_{i=1}^N\sum_{j=1}^N \alpha_i
\alpha_j y_i y_j (x_i \dot x_j) + \sum_{i=1}^N \alpha_i$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;带入刚开始的对偶问题得:&lt;br&gt;&lt;span&gt;$$\max_\alpha \frac12 \sum_{i=1}^N\sum_{j=1}^N \alpha_i
\alpha_j y_i y_j (x_i \dot x_j) - \sum_{i=1}^N \alpha_i\\
s.t. \sum_{i=1}^N \alpha_i y_i = 0 \\
\qquad \alpha_i \ge 0, i = 1,2,\dots,N$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;有最优化知识可知，这两个等价的原始和对偶问题有相同的解，即存在&lt;span&gt;$\alpha^*$&lt;/span&gt;&lt;!-- Has MathJax --&gt;是对偶问题的解，由它而得到&lt;span&gt;$w^*$&lt;/span&gt;&lt;!-- Has MathJax --&gt;是原始问题的解法，得到了&lt;span&gt;$w^*$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;带入一个支持向量则很容易得到&lt;span&gt;$b^*$&lt;/span&gt;&lt;!-- Has MathJax --&gt;的值，由KKT条件可知：&lt;br&gt;&lt;span&gt;$w^* = \sum_{i=1}^N \alpha_i^* y_i x_i$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;代入，故分类平面可以写作：&lt;br&gt;&lt;span&gt;$\sum_{i=1}^N \alpha_i^* y_i (x \dot x_i) + b^* = 0$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;分类决策函数为：&lt;br&gt;&lt;span&gt;$f(x) = sign(\sum_{i=1}^N \alpha_i^* y_i (x \dot x_i) + b^*)$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;SVM是一种无论从研究还是应用上都非常有价值的模型，它的效果非常好，尤其是 在解决小样本、非线性以及高纬数据中表现出了较之以前的方法不曾有的优势， 但是它的基本模型仍然是一个简单的线性分类模型，Max-margin的思想让它 区别于感知器，软间隔和核方法让SVM也同样适用于
    
    </summary>
    
    
      <category term="ML" scheme="http://wallsbreaker.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>漫谈PCA</title>
    <link href="http://wallsbreaker.github.io/2017/01/14/%E6%BC%AB%E8%B0%88PCA/"/>
    <id>http://wallsbreaker.github.io/2017/01/14/漫谈PCA/</id>
    <published>2017-01-14T11:19:35.000Z</published>
    <updated>2017-01-18T05:14:40.000Z</updated>
    
    <content type="html">&lt;p&gt;维度降解是一个在机器学习应用或者数据挖掘领域非常常见的一个方法和概念，它的含义很直白，就是要将数据从原本所在的高维空间映射到低维空间中去。理解维度降解需要从两个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么要做维度降解&lt;/li&gt;
&lt;li&gt;映射到哪里去,如何映射&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;为什么需要维度降解&quot;&gt;&lt;a href=&quot;#为什么需要维度降解&quot; class=&quot;headerlink&quot; title=&quot;为什么需要维度降解&quot;&gt;&lt;/a&gt;为什么需要维度降解&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;维度灾难：很难有一个简洁的模型在高维空间中依旧具有鲁棒性，而随着模型复杂度的增加，为保证结果同样的精度和准确性，所需要的数据也需要极大增加，而高维空间的数据本身具备稀疏性，可想而知如果真要这么做，需要的数据是难以估计的。当然并不是所有的高维空间都不好，比如核方法，有些非线性的规律就需要映射到更高维的空间中去才能更好地解决，只不过核方法提供了一种更为简洁的运算。&lt;/li&gt;
&lt;li&gt;查询和计算的准确性和效率。准确性的话维度越高相对来说的确精度越低，一是数据度量本身的不准确性增加，而是计算时比如浮点数或者舍入等情况越多的发生，对准确性会有影响。而效率的影响更是显而易见的。&lt;/li&gt;
&lt;li&gt;去噪。 降维带来的去燥效果主要来源于两个方面，一是上面所说的计算时准确性同样的原因。二就是我们将维度降下来意味着我们只保留最主要的规律和信息，而那些轻微的细小的相关性一部分而是噪音的影响，另外一些则是我们并不希望关注的数据本身的弱关联性。&lt;/li&gt;
&lt;li&gt;数据压缩。 这主要是从存储的角度考虑&lt;/li&gt;
&lt;li&gt;可视化。 如果有可视化的需要的话,2D,3D&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h1&gt;&lt;p&gt;维度讲解的方法历经发展已经有很多：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear:&lt;br&gt;   Unsupervised: PCA, ICA, SVD, LSA(LSI)&lt;br&gt;   Supervised: LDA, CCA, PLS&lt;/li&gt;
&lt;li&gt;Non-linear:&lt;br&gt;  Unsupervised: LLE, Isomap, MDR&lt;br&gt;  Supervised: Learning with Non-linear kernels&lt;br&gt;这里我们只介绍PCA, LLE和Isomap。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;PCA&quot;&gt;&lt;a href=&quot;#PCA&quot; class=&quot;headerlink&quot; title=&quot;PCA&quot;&gt;&lt;/a&gt;PCA&lt;/h2&gt;&lt;p&gt;其实从本质上讲PCA只是在做线性变换,就将讲原本数据变换到方差最大的那几个新坐标轴上去，尽量将数据中原有的不相关性保留下来。具体的做法就是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减均值（相当于将新坐标系的原点平移到了数据中心）$ X = X - \bar x$&lt;/li&gt;
&lt;li&gt;计算协方差矩阵 $ S = \frac1N \sum_n x_n x_n^T$&lt;/li&gt;
&lt;li&gt;计算top d 特征值对应的特征向量 $U$(这就是新坐标系)&lt;/li&gt;
&lt;li&gt;$Y=U^TX$（相当于旋转到新坐标系）&lt;/li&gt;
&lt;li&gt;重构原始数据$\hat{X} = UY = UU^TX$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于任意一个原坐标系的数据点 $p_x$, $\bar x$为数据均值,则变换后在新坐标系的坐标为：&lt;br&gt;&lt;span&gt;$p_y = U^T (p_x - \bar x)$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;/p&gt;
&lt;h3 id=&quot;如何选取-d&quot;&gt;&lt;a href=&quot;#如何选取-d&quot; class=&quot;headerlink&quot; title=&quot;如何选取$d$&quot;&gt;&lt;/a&gt;如何选取$d$&lt;/h3&gt;&lt;span&gt;$$r_i = \frac{\lambda_i}{\sum_{j=1}^p} * 100%\\\
\sum_{i=1}^d r_i \le threshold$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;
&lt;p&gt;这里阈值则是按经验和问题而定，比如$95\%$。&lt;/p&gt;
&lt;h3 id=&quot;Theory-of-PCA&quot;&gt;&lt;a href=&quot;#Theory-of-PCA&quot; class=&quot;headerlink&quot; title=&quot;Theory of PCA&quot;&gt;&lt;/a&gt;Theory of PCA&lt;/h3&gt;&lt;p&gt;PCA的重要理论主要是集中在如何选取投影矩阵上，为什么要选取特征值大的.主要可以从以下几个角度去衡量。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最大化方差。一个机器学习问题泛泛地讲其实就是给定一个数据集，根据问题从中寻找一些隐含在数据背后的规律和信息,从信息熵的角度上考虑，这些规律和信息都体现在数据的不相关性上也就是方差上，所以我们降维的目的也就可以设置为降解到相同维度上依然能够保留更大的方差。从数学推导中可以求出需要选择最大d个特征值对应的特征向量。&lt;/li&gt;
&lt;li&gt;最小重构误差。 不从方差的角度而是从人类的直觉上来考虑（没错很多方法的思想都是符合人类思维或者是有人类直觉出发灵感而生的），对于维度降解问题，如果降解之后的数据重构会原始数据时，它的误差非常小，即还是能够保留最多的原始信息，那么我们会觉得这个降解是成功的。而PCA得降解是可逆的，这就为这种证明和推导提供了可能，同样从数据上可以证明映射矩阵就是由最大的d个特征值对应的特征向量构成s的。&lt;/li&gt;
&lt;li&gt;概率PCA PPCA becomes PCA when the variance tends to zero&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Dual-PCA-PCA-in-high-dimensions&quot;&gt;&lt;a href=&quot;#Dual-PCA-PCA-in-high-dimensions&quot; class=&quot;headerlink&quot; title=&quot;Dual PCA - PCA in high-dimensions&quot;&gt;&lt;/a&gt;Dual PCA - PCA in high-dimensions&lt;/h3&gt;&lt;p&gt;考虑这样一种情况，$p&amp;gt;&amp;gt;N$，维度非常高，在求$p\times p$的协方差矩阵特征值特征向量的时候具有$O(p^3)$的复杂度，这使得计算的代价非常大。而我们知道由于只有$N$个数据，协方差矩阵只有最多不超过$N$个非零特征值,所有就想如果$S$和$G=X^TX$矩阵的特征值特征向量有关系就好了，由此引出了对偶PCA。&lt;br&gt;对于均值化了的数据X，有：&lt;br&gt;&lt;span&gt;$$S = \frac1N XX^T, \text{有特征值特征向量}(\lambda_i,\mu_i)\\\
G = X^TX, \text{有特征值特征向量}(\gamma_i,\upsilon_i)\\\
\mu_i = \frac{1}{\gamma_i}X\upsilon_i$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;/p&gt;
&lt;h3 id=&quot;Kernel-PCA&quot;&gt;&lt;a href=&quot;#Kernel-PCA&quot; class=&quot;headerlink&quot; title=&quot;Kernel PCA&quot;&gt;&lt;/a&gt;Kernel PCA&lt;/h3&gt;&lt;p&gt;传统的PCA是一种线性映射，也就是说仅仅是通过对坐标系进行平移和旋转操作。如果需要对数据进行非线性的映射，则可以引入核方法：&lt;br&gt;&lt;span&gt;$x \to \phi(x)$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;br&gt;然后应用dual PCA的方法：&lt;br&gt;&lt;span&gt;$G = \phi(X)^T \phi(X) = [k(x_i, x_j)]_{i,j}$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;/p&gt;
&lt;h2 id=&quot;Nonlinear-dimension-Reduction-Manifold-Learning&quot;&gt;&lt;a href=&quot;#Nonlinear-dimension-Reduction-Manifold-Learning&quot; class=&quot;headerlink&quot; title=&quot;Nonlinear dimension Reduction(Manifold Learning)&quot;&gt;&lt;/a&gt;Nonlinear dimension Reduction(Manifold Learning)&lt;/h2&gt;&lt;p&gt;流型是一种局部具有欧几里得空间性质的空间，是欧几里得空间中曲线、曲面的推广，简单的直观理解：高位欧式空间嵌入的被扭曲的子空间，有一个不错的例子就是一块平铺的布，把它看做一个二维平面，在三维空间中我们把它随便扭一扭，它所构成的空间就是一个流型。流型满足一些好的性质，比如连续的结构，考察任何一个局部(切空间)都可以用一个线性模型来无穷逼近它&lt;br&gt;&lt;img src=&quot;/images/lle_1.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;有别于传统欧几里得空间的性质也使得对流型的维度降解有了新的要求，在利用传统的降解方法很容易产生错误，因为我们希望在原本高维空间中靠近的数据点在讲解之后的地位空间中同样靠近。同样有几种方法，这里以LLE和Isomap为例,这两种方法都立足于局部可用欧式距离衡量这一性质。&lt;/p&gt;
&lt;h3 id=&quot;Locally-linear-embedding-LLE&quot;&gt;&lt;a href=&quot;#Locally-linear-embedding-LLE&quot; class=&quot;headerlink&quot; title=&quot;Locally linear embedding(LLE)&quot;&gt;&lt;/a&gt;Locally linear embedding(LLE)&lt;/h3&gt;&lt;p&gt;LLE的核心思想是希望能够保持局部样本的线性关系，在原始空间中每一个样本都可以用局部内的近邻数据点进行线性表示，而降维之后的空间中，仍然希望保持住这种线性表示关系。&lt;br&gt;方法的具体步骤是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;求解K近邻&lt;/li&gt;
&lt;li&gt;最小化重构误差  &lt;span&gt;$$\min_{W} \epsilon(W) = \sum_i ||x_i - \sum_{j\in N_i}W_{ij}x_j||^2\\\
s.t. \sum_j W_ij = 1, \forall i$$&lt;/span&gt;&lt;!-- Has MathJax --&gt;&lt;/li&gt;
&lt;li&gt;子空间线性关系保持  &lt;span&gt;$\min_{Y} \phi(Y) = \sum_i ||y_i - \sum_{j\in N_i}W_{ij}y_j||^2$&lt;/span&gt;&lt;!-- Has MathJax --&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Isomap&quot;&gt;&lt;a href=&quot;#Isomap&quot; class=&quot;headerlink&quot; title=&quot;Isomap&quot;&gt;&lt;/a&gt;Isomap&lt;/h3&gt;&lt;p&gt;Isomap的思想就是将传统的欧氏直线距离换为了流型的测地距离，也就是沿着流型的距离。当然，如果流型的结构事先不知道的话，Isomap通过利用最近邻点连接起来构成一个邻接图来离散的近似原来的流型，测地距离也就相应地通过图上的最短路径来近似,其中一个数据点距离它的最近邻为其二者的欧氏距离，而距离其余点则为无穷大。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;维度降解是一个在机器学习应用或者数据挖掘领域非常常见的一个方法和概念，它的含义很直白，就是要将数据从原本所在的高维空间映射到低维空间中去。理解维度降解需要从两个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么要做维度降解&lt;/li&gt;
&lt;li&gt;映射到哪里去,如何映射&lt;/li&gt;
&lt;/ul
    
    </summary>
    
    
      <category term="ML" scheme="http://wallsbreaker.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的矩阵</title>
    <link href="http://wallsbreaker.github.io/2017/01/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5/"/>
    <id>http://wallsbreaker.github.io/2017/01/07/机器学习中的矩阵/</id>
    <published>2017-01-07T13:07:06.000Z</published>
    <updated>2017-01-18T05:14:20.000Z</updated>
    
    <content type="html">&lt;p&gt;矩阵在现代工程中的应用之广不言而喻，关于矩阵的理解仅仅停留在本科线性代数知道定义会计算的层次是不够的，这里结合自己的理解和学习的内容谈谈矩阵。&lt;/p&gt;
&lt;h1 id=&quot;矩阵是什么&quot;&gt;&lt;a href=&quot;#矩阵是什么&quot; class=&quot;headerlink&quot; title=&quot;矩阵是什么&quot;&gt;&lt;/a&gt;矩阵是什么&lt;/h1&gt;&lt;p&gt;从形式上来看，一维的是向量，将向量按行按列组织起来形成二维的就是矩阵，继续走的话还有张量，当然张量一般研究的不是太多，因为更多的应用还是在矩阵，而且矩阵的好多性质和规律是可以推广到张量的，所以一般的研究主体还是矩阵。那矩阵到底是什么，这还要从向量说起。&lt;br&gt;向量是什么，一个有方向有大小的几何对象，通俗的说是给定坐标系上的一个坐标，空间中的一个点。从工程的角度讲，坐标系就是我们定义好的各种属性维度，实体可以用一个向量刻画，因为我们只要知道这个实体在我们定义好的关心的属性维度上的值就可以了，我们可以带着这个有值得向量做各种想做的事情。而矩阵，不仅可以看做是为了方便多个向量的组织而放在了一起，尤其做矩阵运算的时候，这个时候的矩阵实际上是表示一个线性变换，将空间上的一个点变换到另一个点（从变换坐标系的角度也是一样），变换的主要方式主要包括缩放，旋转，投影。缩放和旋转比较好理解，投影的话则是因为该矩阵不是方阵，变换之后向量的维度发生了变化，空间变了，也就是从该空间投影到了另外的一个空间，其中矩阵的秩的意义即为投影维度的变化。正交矩阵的特殊在于它只是做旋转变换。&lt;br&gt;$$&lt;br&gt;Au = v&lt;br&gt;$$&lt;br&gt;在同一个坐标系上来讲，通过左乘$A$矩阵把$u$变换成了$v$。乘上矩阵的话也是一样，相当于对很多列向量做变换，从这点出发，就可以理解为什么矩阵的乘法要定义成那样做。后续很多矩阵的相关概念立足于矩阵是一种变换这个基础就会比较好理解。&lt;/p&gt;
&lt;h1 id=&quot;特征值与奇异值&quot;&gt;&lt;a href=&quot;#特征值与奇异值&quot; class=&quot;headerlink&quot; title=&quot;特征值与奇异值&quot;&gt;&lt;/a&gt;特征值与奇异值&lt;/h1&gt;&lt;h2 id=&quot;特征值&quot;&gt;&lt;a href=&quot;#特征值&quot; class=&quot;headerlink&quot; title=&quot;特征值&quot;&gt;&lt;/a&gt;特征值&lt;/h2&gt;&lt;p&gt;$$&lt;br&gt;Au = v&lt;br&gt;$$&lt;br&gt;向量$u$和$v$有没有没有可能恰好同向或反向呢？答案是有可能的，也就是我们学过的特征值的定义：&lt;br&gt;$$&lt;br&gt;Av = \lambda v&lt;br&gt;$$&lt;br&gt;这中方向$v$的个数是有限的，当矩阵$A$作用于这些方向时，就相当于只是对其做了一个缩放，当然这个缩放有正有负，也有可鞥是0。因为这些方向的稀有性，我们可以把他们单独拿出来做研究，是不是有什么良好的性质或用途呢？起个名字吧，叫特征向量，当然也有翻译成本征向量的，其实这个名字更形象，矩阵既然是个变换，这种具有不变形的方向就是它的本质属性之一嘛，缩放的比例也就称为特征值或本征值。特征值越大说明矩阵在这个方向上的能量越大，PCA中选取特征值大的特征向量做投影就是希望能够尽量保持足够大的信息量。&lt;/p&gt;
&lt;h2 id=&quot;奇异值&quot;&gt;&lt;a href=&quot;#奇异值&quot; class=&quot;headerlink&quot; title=&quot;奇异值&quot;&gt;&lt;/a&gt;奇异值&lt;/h2&gt;&lt;p&gt;特征值和特征向量都是对方阵而言的，不是方阵的话有没有这种能够衡量矩阵属性的概念呢？也是有的，那就是奇异值，任何矩阵都可以析构为这样一种形式：&lt;br&gt;$$&lt;br&gt;A = \mu \Sigma \sigma ^T&lt;br&gt;$$&lt;br&gt;这成为奇异值分解，其中$\mu$和$sigma$为正交矩阵，$\Sigma$为只含对角线元素的矩阵，对角线的值为奇异值，它表示$A$矩阵的作用是将一个向量从$\sigma$这组基变换到$u$这组基向量空间中去，这个变换包含了旋转、缩放和投影，其中奇异值也就缩放因子，其中不为0的个数为矩阵的秩。因为矩阵的行列式的绝对值（体积）为所有奇异值的积，如果有一个为0，矩阵则奇异，这也就是命名为奇异值的原因。其中矩阵的谱范数就是最大奇异值&lt;br&gt;事实上，奇异值刻画了矩阵的奇异性，也就是其行或列的线性相关性，相关性越强，则非奇异矩阵接近奇异的，最小的奇异值越小，条件数(最大最小奇异值之比)越大，如果其行或列相关了，那么矩阵也就变成了奇异的，最小的奇异值是0，条件数无穷大。一个矩阵的奇异值是固定的，而左右奇异向量则是不定的，可以找到多种形式。&lt;/p&gt;
&lt;h2 id=&quot;特征值与奇异值的异同&quot;&gt;&lt;a href=&quot;#特征值与奇异值的异同&quot; class=&quot;headerlink&quot; title=&quot;特征值与奇异值的异同&quot;&gt;&lt;/a&gt;特征值与奇异值的异同&lt;/h2&gt;&lt;p&gt;首先从形式上来说，奇异值适用于任何矩阵，而特征值则只是针对方阵而言的。事实上，即使对于同一个方阵来说，特征值与奇异值之间也没有什么内在的关系，但是$m \times n$的非零奇异值是$A^TA$或$AA^T$非零特征值的正平方根。&lt;/p&gt;
&lt;h1 id=&quot;特征值分解与奇异值分解&quot;&gt;&lt;a href=&quot;#特征值分解与奇异值分解&quot; class=&quot;headerlink&quot; title=&quot;特征值分解与奇异值分解&quot;&gt;&lt;/a&gt;特征值分解与奇异值分解&lt;/h1&gt;&lt;p&gt;特征值分解或奇异值分解有很多用途，比如节省存储（分解成了三个小矩阵）、降维(如上一节PCA所述)、去噪（小奇异值很有可能是噪音，去掉了会更好）、推荐(SVD)等。另外， 以特征值分解为例，可以更好帮助我们理解特征向量为什么刻画了矩阵的本质方向，为了更好的说明，以对称矩阵（总能相似对角化，特征向量相互正交）为例。&lt;br&gt;当个特征值和特征向量有：&lt;br&gt;$$&lt;br&gt;A x_i = \lambda_i x_i&lt;br&gt;$$&lt;br&gt;全部特征值和特征向量写成矩阵的形式则是：&lt;br&gt;$$&lt;br&gt;AU = U\wedge&lt;br&gt;$$&lt;br&gt;其中$U$作为正交矩阵，所以可以写为：&lt;br&gt;$$&lt;br&gt;A = U\wedge U^{-1} = U\wedge U^T&lt;br&gt;$$&lt;br&gt;当矩阵用作线性变化时，有：&lt;br&gt;$$&lt;br&gt;Ax = U\wedge U^Tx&lt;br&gt;$$&lt;br&gt;其中向量$x$可以用$U$作为坐标系表示为：&lt;br&gt;$$&lt;br&gt;x = a_1x_1 + a_2x_2 + \dots + a_mx_m&lt;br&gt;$$&lt;br&gt;所以有&lt;br&gt;$$&lt;br&gt;U\wedge U^T x = U\wedge[a_1, a_2, \dots, a_m]^T\\&lt;br&gt;=U[\lambda_1a_1, \lambda_2a_2, \dots, \lambda_ma_m]^T&lt;br&gt;$$&lt;br&gt;写到这，就可以更清楚地借助特征值特征向量看出矩阵到底在做一个什么样的变化。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;p&gt;矩阵在现代工程中的应用之广不言而喻，关于矩阵的理解仅仅停留在本科线性代数知道定义会计算的层次是不够的，这里结合自己的理解和学习的内容谈谈矩阵。&lt;/p&gt;
&lt;h1 id=&quot;矩阵是什么&quot;&gt;&lt;a href=&quot;#矩阵是什么&quot; class=&quot;headerlink&quot; title=&quot;矩阵是
    
    </summary>
    
    
  </entry>
  
</feed>
